{"cells":[{"cell_type":"markdown","metadata":{},"source":"# Reinforcement Learning\n## Q-Learning \n\nThe goal here is to implement a Reinforcement Learning algorithm: Q-learning.\n### Description : \nAn robot/agent can move in a m*n maze. The environment can have a random component: with each choice of movement (up, down, left or right), the agent can move in an unwanted direction. This behavior is configurable.\n\n### Credits\nBased on [Akka Zemmari](https://www.labri.fr/perso/zemmari/) RL courses at [ENSEIRB-MATMECA](https://enseirb-matmeca.bordeaux-inp.fr/fr)\n\nQ-learning implemention by Fabien Couthouis."},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"import random\nclass Game:\n    ACTION_UP = 0\n    ACTION_LEFT = 1\n    ACTION_DOWN = 2\n    ACTION_RIGHT = 3\n    \n    ACTIONS = [ACTION_UP, ACTION_LEFT, ACTION_DOWN, ACTION_RIGHT]\n    ACTIONS_NAMES = ['UP','LEFT','DOWN','RIGHT']\n    \n    MOVEMENTS = {\n        ACTION_UP: (1, 0),\n        ACTION_RIGHT: (0, 1),\n        ACTION_LEFT: (0, -1),\n        ACTION_DOWN: (-1, 0)\n    }\n    \n    num_actions = len(ACTIONS)\n    \n    def __init__(self, n, m, nb_blocks,wrong_action_p=0.1, alea=False):\n        self.n = n\n        self.m = m\n        self.nb_blocks = nb_blocks\n        self.wrong_action_p = wrong_action_p\n        self.alea = alea\n        self.generate_game()\n        \n    def _position_to_id(self, x, y):\n        \"\"\"Return the square id\"\"\"\n        return x + y * self.n\n    \n    def _id_to_position(self, id):\n        \"\"\"Return the square corresponding to the given id\"\"\"\n        return (id % self.n, id // self.n)\n    \n    def generate_game(self):\n        \"\"\"Random game generation\"\"\"\n        cases = [(x, y) for x in range(self.n) for y in range(self.m)]\n        start = random.choice(cases)\n        cases.remove(start)\n        end = random.choice(cases)\n        cases.remove(end)\n\n        self.blocks = []\n        for b in range(self.nb_blocks):\n            block = random.choice(cases)\n            cases.remove(block)\n            self.blocks.append(block)\n        \n        self.position = start\n        self.end = end\n        \n        self.counter = 0\n        \n        if not self.alea:\n            self.start = start\n        return self._get_state()\n    \n    def reset(self):\n        if not self.alea:\n            self.position = self.start\n            self.counter = 0\n            return self._get_state()\n        else:\n            return self.generate_game() \n    \n    def _get_grid(self, x, y):\n        grid = [\n            [0] * self.n for i in range(self.m)\n        ]\n        grid[x][y] = 1\n        return grid\n    \n    def _get_state(self):\n        if self.alea:\n            return [self._get_grid(x, y) for (x, y) in\n                    [self.position, self.end, self.block]]\n        return self._position_to_id(*self.position)\n   \n    def move(self, action):\n        \"\"\"\n        Move the agent \n        :param action : action id\n        :return ((state_id, end, hole, block), reward, is_final, actions)\n        \"\"\"\n        self.counter += 1\n        if action not in self.ACTIONS:\n            raise Exception('Invalid action')\n        \n        choice = random.random()\n        if choice < self.wrong_action_p :\n            action = (action + 1) % 4\n        elif choice < 2 * self.wrong_action_p:\n            action = (action - 1) % 4\n            \n        d_x, d_y = self.MOVEMENTS[action]\n        x, y = self.position\n        new_x, new_y = x + d_x, y + d_y\n        \n        if (new_x, new_y) in self.blocks :\n            return self._get_state(), -1, False, self.ACTIONS\n        elif self.end == (new_x, new_y):\n            self.position = new_x, new_y\n            return self._get_state(), 10, True, self.ACTIONS\n        elif new_x >= self.n or new_y >= self.m or new_x < 0 or new_y < 0:\n            return self._get_state(), -1, False, self.ACTIONS\n        elif self.counter > 190:\n            self.position = new_x, new_y\n            return self._get_state(), -10, True, self.ACTIONS\n        else:\n            self.position = new_x, new_y\n            return self._get_state(), -1, False, self.ACTIONS\n        \n    def print(self):\n        str = \"\"\n        for i in range(self.n - 1, -1, -1):\n            for j in range(self.m):\n                if (i, j) == self.position:\n                    str += \"x\"\n                elif (i, j) in self.blocks:\n                    str += \"¤\"\n                elif (i, j) == self.end:\n                    str += \"@\"\n                else:\n                    str += \".\"\n            str += \"\\n\"\n        print(str)"},{"cell_type":"markdown","metadata":{},"source":"It is assumed that the environment is fixed: the position of the elements will not be changed between each part. It is therefore a matter of learning the structure of the ground, to be able to move the agent correctly.\n\nIn the next part, we will implement Q-learning with table and with a simple neural network.\n"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"/home/fabien/.miniconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/home/fabien/.miniconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/home/fabien/.miniconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/home/fabien/.miniconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/home/fabien/.miniconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/home/fabien/.miniconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n/home/fabien/.miniconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/home/fabien/.miniconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/home/fabien/.miniconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/home/fabien/.miniconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/home/fabien/.miniconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/home/fabien/.miniconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"}],"source":"import numpy as np \nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, InputLayer\n\nclass Agent:\n    \"\"\"\n    Parameters:\n        game: Game object\n        use_nn, optionnal (default=False): If True, train agent using a neural network, use Q-table otherwise\n    \"\"\"\n\n    def __init__(self,game, use_nn=False):\n        self.game = game\n        self.num_states = self.game.n * self.game.m\n        self.num_actions = Game.num_actions\n        self.use_nn = use_nn\n        self.max_training_iterations = self.num_states ** 3\n\n        if self.use_nn:\n            self.model = self._build_model()\n        else:\n            self.Q = np.zeros([self.num_states, self.num_actions])\n\n\n\n    def _build_model(self):\n        \"\"\"Build nn keras model\"\"\"\n        model = Sequential()\n        model.add(InputLayer(batch_input_shape=(1,self.num_states)))\n        model.add(Dense(8, activation='sigmoid'))\n        model.add(Dense(self.num_actions, activation='linear'))\n        #loss = ( reward + y * max( Q(s',a') - Q(s,a) ) )^2 \n        model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n\n        return model\n\n\n    def _act(self,s):\n        \"\"\"Select next action\"\"\"\n        if self.use_nn:\n            #np.identity is used to one hot encode state s\n            a = np.argmax(self.model.predict(np.identity(self.num_states)[s:s + 1]))\n        else:\n            Q2 = self.Q[s,:]\n            a = np.argmax(Q2)\n\n        return a\n\n\n    def _train_one_episode(self,lr,y):\n        actions = []\n        s = self.game.reset()\n        states = []\n        cumul_reward = 0\n        nb_iterations = 0\n        done = False\n\n        while not done and nb_iterations < self.max_training_iterations:\n            a = self._act(s)\n            new_s,reward,done,_ = self.game.move(a)\n\n            if self.use_nn:\n                target = reward + y * np.max(self.model.predict(np.identity(self.num_states)[new_s:new_s + 1]))\n                target_vector = self.model.predict(np.identity(self.num_states)[s:s + 1])[0]\n                target_vector[a] = target\n                self.model.fit(np.identity(self.num_states)[s:s + 1], target_vector.reshape(-1, self.num_actions), epochs=1,  verbose=False)\n\n            else:\n                self.Q[s,a] = self.Q[s,a] + lr * (reward + y * np.max(self.Q[new_s,:]) - self.Q[s,a])\n\n            cumul_reward += reward\n            s = new_s\n            actions.append(a)\n            states.append(s)\n            nb_iterations += 1\n        \n        return states,actions,cumul_reward, nb_iterations\n\n\n\n    def train(self,num_episodes = 1000, lr=0.85,y=0.99, use_nn=False):\n        cumul_rewards_list,actions_list,states_list = [],[],[]\n\n        print(\"Start of the game\")\n        self.game.print()\n\n\n        for t in range(num_episodes):\n            actions, states, cumul_reward, nb_iterations = self._train_one_episode(lr,y)\n\n            actions_list.append(actions)\n            states_list.append(states)\n            cumul_rewards_list.append(cumul_reward)\n\n            #No solution\n            if nb_iterations >= self.max_training_iterations:\n                print(\"No solution found after {} iterations\".format(nb_iterations))\n                break\n\n        print(\"\\nEnd of the game\")\n        self.game.print()\n        print(\"Score over time: {}\".format(sum(cumul_rewards_list[-100:])/100.0))"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Start of the game\n........\n.¤.¤.¤¤.\n.¤...¤@.\n¤..¤.x..\n¤.......\n¤¤.¤....\n........\n.¤¤..¤..\n\n\nEnd of the game\n........\n.¤.¤.¤¤.\n.¤...¤x.\n¤..¤....\n¤.......\n¤¤.¤....\n........\n.¤¤..¤..\n\nScore over time: 0.68\n"}],"source":"game = Game(8,8,15)\nagent = Agent(game, use_nn=True)\nagent.train(num_episodes=100)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}